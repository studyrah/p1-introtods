Short Questions to Analyzing the NYC Subway Dataset

Analyzing the NYC Subway Dataset

Short Questions


  Overview

For Part 1 of the project, you should have completed the questions in
Problem Sets 2, 3, 4, and 5 in the Introduction to Data Science course.

For part 2 of the project, please use this document as a template and
answer the following questions to explain your reasoning and conclusion
behind your work in the problem sets.


  Section 1. Statistical Test

 1. Which statistical test did you use?

Mann Whitney U Test

 2. Why is this statistical test appropriate or applicable to the dataset?

The histogram of ENTRIESn_hourly does not show the typical bell like shape of a normal/gaussian distribution (not even skewed), instead it shows a pattern of consistent reduction in frequency from one bucket to the next

So we shouldn't perform a test such as the Students T test which assumes a normal distribution. The Mann Whitney U test does not assume any particular distribution and can tell you whether there is a statistically significant difference between two sample means

 3. What results did you get from this statistical test?

Rainy Day Mean = 1105.4463767458733
Non Rainy Day Mean = 1090.278780151855
Test Statistic = 1924409167.0
P Value = 0.024999912793489721

 4. What is the significance of these results?

Here we had a null hypothesis that the mean of the two sets is equal but we received a p value of ~ 0.02, less than the typical p critical of 0.05 so we reject the null hypothesis and assert there is a statistically significant difference between the mean of the two sets.  Rainy days had the higher mean so we expect that more people (on average) go through the turnstiles on rainy days.



  Section 2. Linear Regression

 1. What approach did you use to compute the coefficients theta and
    produce prediction in your regression model:

 1. Gradient descent (as implemented in exercise 3.5)
 
 YES
 
 2. OLS using Statsmodels
 3. Or something different?

 2. What features did you use in your model? Did you use any dummy
    variables as part of your features?

Gradient Descent:

rain
precipi
Hour
meantempi
UNIT
ones
weekday - whether or not it was a weekday or weekend

UNIT was dummy coded but this was provided.

for weekday I used 1 for weekday and 0 for weekend.  I also experimented with dummy coding day of the week (0 - 6) but this proved to be less effective in practice


 3. Why are these features appropriate?


Gradient Descent:

rain, precipi - intuitively rain and the degree of rain will affect whether people choose to walk, cycle, ... or use the subway

hour - density of travel fluctuates with 'typical working patterns' and 'typical sleeping patterns' for instance

meantempi - again intuitively this will affect peoples choice of transport (& whether they go out at all)

UNIT - unit is tightly linked with station, clearly different stations (based upon geography, attractions, population density etc.) can have there own typical patterns of entry

weekday - a reflection of the working week, more people work Mon - Fri than on Sat/Sun.  Calculating simple averages on the dataset backs this intuition up.


 4. What is your model’s R2 (coefficients of determination) value?
 
 
 Gradient Descent: 
 
 R2 = 0.474350279

 5. What does this R2 value mean for the goodness of fit for your
    regression model? Do you think this linear model is appropriate for
    this dataset, given this R2  value?

We know 0 is bad (explains none of the error) and 1 is perfect prediction. So is my value of 0.47 good or bad? Hard to say

It's significantly better than the ~ -0.2 we got with the given model (including parameters) but ...

For a random 15% set of the data I first plotted residuals which does give a nicely normal distribution peaking around 0 error, so at first glance it looks pretty good.

#####
see - ......p3-projects/p4-gradientdescent-residuals.png
#####

I then plotted observed values alongside predicted values as follows:

#####
see - ......p3-projects/p3-gradientdescent-observed-vs-predicted.png
#####

You could conclude that the model does quite well when ENTRIESn_hourly > 300 (ish), for less than that the model is very poor.  Looking at the observed values there is a conspicously high frequency of observations for ENTRIESn_hourly < 10 (ish).  Could this be situations stations are closed, either routinely or for maintenance etc.? If so and we can characterise these occurrences into features (either by acquiring extra data or 'better understanding the data we have got' then perhaps we can dramatically improve our model for these cases too.



################################################################################


  Section 3. Visualization

Please include two visualizations that show the relationships between
two or more variables in the NYC subway data. You should feel free to
implement something that we discussed in class (e.g., scatterplots, line
plots, or histograms) or attempt to implement something more advanced if
you'd like.



On ggplot:

I found this project particularly frustrating due in no small measure to the python ggplot library.  I wasted a lot of time trying to do (arguably) more interesting visualisations combining the turnstile/weather data with station reference data only to be thwarted by missing functionality such as:

- rotate axis labels
- displaying legends
- ordering bar charts by y val
- fill/colour by categorical values
- stack bars by categorical values
- scatter plots with categorical x values
- ...

Several of these are known bugs/missing features and either in or scheduled for future releases.  At this point in time though I would say that the stable release is 'more trouble than its worth'



 1. One visualization should be two histograms of ENTRIESn_hourly for
    rainy days and non-rainy days
    
#####
see - ......p3-projects/p1_rain_vs_no_rain_histogram.png
#####

Shows, as we might expect that there are more data points for dry weather (roughly twice as many) but that the distributions for rainy vs non-rainy days are similar.

For fun I thought I'd also take a look at the more extreme cases (i.e. very high number of entries) to see if the pattern remained the same. That is to say, was it still the case that the frequency of dry day observations was roughly double that of rainy day cases or was the frequency of rainy day cases now higher

#####
see - ......p3-projects/p1_rain_vs_no_rain_high_histogram.png
#####

Inconclusive (to the eye)




 2. One visualization can be more freeform, some suggestions are:

 1. Ridership by time-of-day or day-of-week
 2. How ridership varies by subway station
 3. Which stations have more exits or entries at different times of day


Ridership by day-of-week

###
see - ......p4-projects/p4_vis22.png
###

I additionally used a colour gradient scale to highlight mean temperature (low = blue, high = red).

You can see that ridership is clearly higher during weekdays compared to weekends and indeed it appears to peak towards the middle of the working week.  In truth if this was all I wanted to show it would have been clearer to just plot daily averages (or bar chart) rather than every data point.

At a glance it looks like there is a tendency for the higher data points to be on days with a lower mean temperature, so do more people use the subway on colder days?  Maybe but by way of a cautionary tale, simply changing the low and high colours can lead you to think the exact opposite!!!

Note that for this visualisation I had hoped to utilise the geom_jitter layer to spread the densely populated data points but the spread was proportionate to the x axis value (so day 6 was markedly more spread than day 0 for example).  The R ggplot2 library has an api to control the spread, it doesn't look like the python implementation does yet.  As a consequence I implemented my own jitter, tailored from - http://nbviewer.ipython.org/gist/fonnesbeck/5850463



Addendum:

I tried to illustrate the 'pattern of subway ridership throughout the course of a day'. That is to say plotting the average frequency of subway entries by hour over the course of the data period.

I expected to see slightly different patterns for weekends vs week days (perhaps right shifted on a weekend & without such obvious peaks) so I categorised the data accordingly and plotted:

###
see - ......p4-projects/p4_vis1.png
###

To my surprise whilst the plot gave a sense of the peaks and troughs you might expect during the day (if you squint a bit), closer inspection showed very sharp peaks and troughs from one hour to the next that 'intuitively did not make sense'.

I consulted the key to the original dataset (http://web.mta.info/developers/resources/nyct/turnstile/ts_Field_Description_pre-10-18-2014.txt), which suggested that audit events occurred every 4 hours (not hourly).  The peaks/troughs started to make sense.

I then performed some cursory analysis of the given dataset and determined that most 'units' were indeed audited at 4 hourly intervals - some at 0,4,8,12,16,20 and others at 1,5,9,13,17,21.  To confuse things further some units were audited every hour. Cross referencing with the unit - station mapping data (http://web.mta.info/developers/resources/nyct/turnstile/Remote-Booth-Station.xls), it became clear that the 'PTH' division alone was performing hourly audits.

I abandoned this question at this point because I couldn't easily reconcile the data to provide a trustworthy output but it occurred to me that analysis would be much easier if they were to standarise audit periodicity and timing.

It also occurred to me that it would be an interesting excercise to try and smooth/standardise the data.  Maybe I'll revisit in a later project....






################################################################################




  Section 4. Conclusion


 1. From your analysis and interpretation of the data, do more people ride
    the NYC subway when it is raining versus when it is not raining?  

YES



 2. What analyses lead you to this conclusion?

Mann Whitney U Test suggesting that there was a statistically significant difference between the means of enrties whilst raining vs not raining, coupled with a higher mean for the raining case.

However, inspite of this conclusion the difference between the means is so small (~ 0.015%) that I'm suspicious!



################################################################################




  Section 5. Reflection


 1. Please discuss potential shortcomings of the data set and the methods
    of your analysis. 

* More data:  Here we have considered data from only one month and for only one year.  It would therefore be dangerous to make any generalised statements about ridership (particularly relating to weather) about other times of the year AND even for that month the weather (for example) may have been atypical for the time of year.
    
* Audit periodicity mismatch:  I've already discussed the 'audit periodicity mismatch' between units (more accurately divisions and stations). This makes drawing conclusions about patterns of use over the course of a day difficult.  Reccomend - audits occur at a consistent time and frequency accross the subway network (preferably hourly)

* Binary rainy vs non-rainy:  For a given day we are told whether it was rainy or not, this coupled with the ammount of rain can enable us to derive models for day by day analysis BUT it would be useful to have a more fine grained time record (e.g. hourly, 4 hourly) of when it was raining.

* Dirty data:  I have performed little to no checking for (and thus cleansing of) false, missing, innacurate data points.  In this instance the data is pretty well controlled and clean (I've mostly got away with it)

* Outliers:  I didn't look for and handle outliers that might be skewing my models and conclusions.  I have already discussed what appears to be a special case where (maybe) stations are closed and thus reporting 0 entries.  Reccomend - Get supplementary data detailing station closures whether due to opening hours, maintenance etc. we could then make a feature of this to improve our machine learning algorithm for example

* Overfitting: I gave little to no consideration of overfitting in my 'linear regression model using gradient descent'. Reccomend - More testing, I could have chosen more random samples for training and test sets and plotted the effectiveness (using techniques already covered - e.g. plotting residuals) to help determine whether overfit was a significant problem.  Is so then I could introduce a Regularisation term to the model.

* & many more ....


Published by Google Drive <//docs.google.com/>–Report Abuse
<//docs.google.com/abuse?id=16T3kirC0IxvtfxlZb7n5kOz5xFF_JTwrG31J2OZj8KM>–Updated
automatically every 5 minutes
